# Visualize-feature-maps



**欢迎关注公众号CV技术指南，专注于计算机视觉的技术总结、最新技术跟踪、经典论文解读、CV招聘信息。**



**特征图可视化是很多论文所需要做的工作，其作用可以是用于证明方法的有效性，也可以是用来增加工作量，给论文凑字数**。

具体来说就是可视化两个图，使用了新方法的和使用之前的，对比有什么区别，然后看图写论文说明新方法体现的作用。

吐槽一句，有时候这个图 论文作者自己都不一定能看不懂，虽然确实可视化的图有些改变，但并不懂这个改变说明了什么，反正就吹牛，强行往自己新方法编的故事上扯，就像小学一年级的作文题--看图写作文。

之前知乎上有一个很热门的话题，如果我在baseline上做了一点小小的改进，却有很大的效果，这能写论文吗？

这种情况最大的问题就在于要如何写七页以上，那一点点的改进可能写完思路，公式推理，画图等内容才花了不到一页，剩下的内容如何搞？可视化特征图！！！

这一点可以在我看过的甚多论文上有所体现，反正我是没看明白论文给的可视化图，作者却能扯那么多道道。这应该就是用来增加论文字数和增加工作量的。

总之一句话，**可视化特征图是很重要的工作，最好要会**。



# Visualize-Heatmap

本文介绍了CAM、GradCAM的原理和缺陷，介绍了如何使用GradCAM算法实现热力图可视化，介绍了目标检测、语义分割、transformer模型等其它类型任务的热力图可视化。


#### 热力图可视化方法的原理

在一个神经网络模型中，图片经过神经网络得到类别输出，我们并不知道模型是根据什么来作出预测的，换言之，我们需要了解图片中各个区域对模型作出预测的影响有多大。这就是热力图的作用，它通过得到图像不同区域之间对模型的重要性而生成一张类似于等温图的图片。

![热力图](F:\Desktop\热力图.png)

热力图可视化方法经过了从CAM，GradCAM，到GradCAM++的过程，比较常用的是GradCAM算法。

##### CAM

CAM论文：Learning Deep Features for Discriminative Localization

CAM的原理是取出全连接层中得到类别C的概率的那一维权值，用W表示。然后对GAP前的feature map进行加权求和，由于此时feature map不是原图像大小，在加权求和后还需要进行上采样，即可得到Class Activation Map。

![image](https://github.com/CV-Tech-Guide/Visualize-feature-maps-and-heatmap/blob/main/images/CAM.png)

CAM有个很致命的缺陷，它的结构是由CNN + GAP + FC + Softmax组成。也就是说如果想要可视化某个现有的模型，对于没有GAP的模型来说需要修改原模型结构，并重新训练，相当麻烦，且如果模型很大，在修改后重新训练不一定能达到原效果，可视化也就没有意义了。

因此，针对这个缺陷，其后续有了改进版Grad-CAM。



##### GradCAM

Grad-CAM论文：Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization

Grad-CAM的最大特点就是不再需要修改现有的模型结构了，也不需要重新训练了，直接在原模型上即可可视化。

<img src="F:\Desktop\GradCAM.png" alt="GradCAM" style="zoom:80%;" />

原理：同样是处理CNN特征提取网络的最后一层feature maps。Grad-CAM对于想要可视化的类别C，使最后输出的类别C的概率值通过反向传播到最后一层feature maps，得到类别C对该feature maps的每个像素的梯度值，对每个像素的梯度值取全局平均池化，即可得到对feature maps的加权系数alpha，论文中提到这样获取的加权系数跟CAM中的系数的计算量几乎是等价的。接下来对特征图加权求和，使用ReLU进行修正，再进行上采样。

使用ReLU的原因是对于那些负值，可认为与识别类别C无关，这些负值可能是与其他类别有关，而正值才是对识别C有正面影响的。

具体公式如下：

<img src="F:\Desktop\公式.png" alt="公式" style="zoom:50%;" />

Grad-CAM后续还有改进版Grad-CAM++，其主要的改进效果是定位更准确，更适合同类多目标的情况，所谓同类多目标是指一张图像中对于某个类出现多个目标，例如七八个人。改进方法是对加权系数的获取提出新的方法，该方法很复杂，这里不介绍。

